---
title: "Introduction to modelling with caret and XGBoost"
output: 
  learnr::tutorial:
    toc: true
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
library(learnr)
library(caret)
library(xgboost)
library(courses.it4i)
library(recipes)
library(modeldata)
data("credit_data")
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.completion = TRUE)

# train_ind <- caret::createDataPartition(heart_small$HeartDisease, p = 0.7, list = FALSE)
# train <- heart_small[train_ind, ]
# test <- heart_small[-train_ind, ]
# 
# ctrl <- caret::trainControl(method = "cv",
#                      number = 5,
#                      classProbs = TRUE,
#                      summaryFunction = twoClassSummary
#                      )
# 
# tune_params <- expand.grid(colsample_bytree = 0.7,
#                           eta = 0.3,
#                           subsample = c(0.5),
#                           nrounds = 100,
#                           gamma = c(0.5),
#                           min_child_weight = c(1),
#                           max_depth = 6)
# rec_obj <- recipes::recipe(HeartDisease ~ ., data = train) |>
#   recipes::step_center(c(2, 6, 7, 15)) |> 
#   recipes::step_scale(c(2, 6, 7, 15)) 
# set.seed(5627)
# xgb_model_clean <- train(rec_obj,
#                          data = train,
#                          method = "xgbTree",
#                          metric = "ROC",
#                          trControl = ctrl,
#                          verbosity = 0,
#                          tuneGrid = tune_params)
# 
# ctrl$sampling <- "rose"
# 
# set.seed(5627)
# xgb_model_rose <- train(rec_obj,
#                    data = train,
#                    method = "xgbTree",
#                    metric = "ROC",
#                    trControl = ctrl,
#                    verbosity = 0,
#                    tuneGrid = tune_params
#                    ) 
# 
# ctrl$sampling <- "smote"
# 
# set.seed(5627)
# xgb_model_smote <- train(rec_obj,
#                    data = train,
#                    method = "xgbTree",
#                    metric = "ROC",
#                    trControl = ctrl,
#                    verbosity = 0,
#                    tuneGrid = tune_params
#                    ) 

```
## Content of this training

-   Why use caret for modelling
-   Data
-   Define model training and tuning
-   Handling class imbalance
-   Prediction and comparison
-   Cheatsheet
-   Exercises

## Why use caret for modelling

Caret is a framework to create a unified workflow for model training with many different models and helper functions to make the process reproducible.

Caret contains:

- Helper functions to preprocess data
- Automatic feature selection for many models
- Cross-validation support
- Hyper-parameter search support
- Model evaluation functions
- Ensemble support
- Parallelization

## Data

It is important to note that the data preprocessing and data transformation depends heavily on the models we are going to use.
Each model have different characteristics, and while some can handle NA values, or correlated variables, others cannot.
Therefore, it is good to understands the strengths and weaknesses of each model and adjust preprocessing routine accordingly.

### Data preprocessing

The data preprocessing is at least in my opinion a stage, where we are trying to get the data into the format which at the end will be used for the partitioning into the training and holdout set.

- Prepare data for the format of the training.
- Creating training and holdout set.
- One-hot encoding (Dummy variables creation).

### Data transformation

- Removing zero- and near zero-variance Variables
- Remove correlated variables
- Remove linear dependencies
- Center and scale the variables
- Impute missing observations
- Dimensionality reduction

From the caret viewpoint there are basically three main approaches to the data preprocessing:

- The first one is to create Your own preprocessing routing and manually handle the preprocessing for the training and testing data.
- The second is to use a parameter `preprocess` in the ```train()``` function of [caret](https://github.com/topepo/caret).
- The third is usage of the custom pre-processing using the [recipes](https://recipes.tidymodels.org).

We will focus on using [recipes](https://recipes.tidymodels.org) as it is the best combination of reproducibility and flexibility.
<!-- For the sake of simplicity I would recommend using the second approach when possible. -->
<!-- However, in real world it is quite likely there are some preprocessing steps that are not handled by the caret package. -->
<!-- Also, the order of operations in the [caret](https://github.com/topepo/caret) is fixed, what might be undesireable sometimes. -->
<!-- In case the [caret](https://github.com/topepo/caret) does not handle Your preprocessing. -->
<!-- You either need to make part of it manually and the other part in the caret package, or use [recipes](https://recipes.tidymodels.org) package which is supported by [caret](https://github.com/topepo/caret). -->
<!-- In case of using [recipes](https://recipes.tidymodels.org) You just need to put the object created by [recipes](https://recipes.tidymodels.org) into the ```train()``` function and then put data as a second argument. -->
<!-- We will not cover usage of the [recipes](https://recipes.tidymodels.org) package in this tutorial, but I recommend checking it out. -->
<!-- It can streamline Your work even if You decide not to use [caret](https://github.com/topepo/caret) in Your workflow. -->

### Key Indicators of Heart Disease dataset

For the next part of this tutorial we will use small version of the Key Indicators of Heart Disease dataset from the [Kaggle](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease).
This dataset contains 18 variables from 2000 people describing their life habits.
Now we will take a look at dataset itself.
The target variable is HeartDisease, which indicates whether given person has a heart disease. The dataset was already converted for the use with the [caret](https://github.com/topepo/caret) and [xgboost](https://xgboost.readthedocs.io/en/stable/R-package/index.html) package.
[Caret](https://github.com/topepo/caret) expects a target variable to be a factor in case of a classification problem, so the `HeartDisease` column is a factor.
On the other hand [xgboost](https://xgboost.readthedocs.io/en/stable/R-package/index.html) expects data to be numeric, so all the other variables are converted into numeric.

```{r str, exercise = TRUE}
str(heart_small)
```

```{r skim, exercise = TRUE}
skimr::skim(heart_small)
```

### Creating a training and testing dataset

Since the dataset is already prepared for the modelling we will focus on creation of the training and holdout sets.
Anyway, there are many other operations that can be made on the data before learning the model, but most of these is best to be done as part of the resampling of the data during the learning.
The reasoning behind this is, that it is necessary to handle the data as in real world case and that means, that any of the operations on the data should be done directly on the subsamples used for training the model, otherwise we would affect also the test data, or holdout data by such transformations and vice versa the training data would be affected also by the knowledge contained in the test data or holdout data.
The caret package stores the values for the transformations done on the training data such as centering, scaling, pca, etc.
This way the transformation done on the test data and holdouet set is the same as on the training data.

Another important aspect to be considered when creating the training and holdout sets is that often the distribution of classes is not symmetrical.
Therefore it is good to make the subsamples in such a way, that the distribution of classes in the training set and holdout set is approximately the same.
Luckily, the function ```caret::createDataPartition``` will consider the class distribution for us and returns indices in such a way so this distribution is kept.

```{r heart_split_data}
train_ind <- caret::createDataPartition(heart_small$HeartDisease, p = 0.7, list = FALSE)
train <- heart_small[train_ind, ]
test <- heart_small[-train_ind, ]
```

## Define model training and tuning

### Setup data transformation

We will use [recipes](https://recipes.tidymodels.org) package to define the transformations that should be made on the data.
Here we will have only two steps, centering and scaling, however [recipes](https://recipes.tidymodels.org) contains many more functions.
Among them are also imputation of values for the NAs.
Another thing that the [recipes](https://recipes.tidymodels.org) can do is add roles to variables. That way some of the variables does not need to be used during training, but may be used to check the predictive ability in deeper way.

```{r recipes}
rec_obj <- recipes::recipe(HeartDisease ~ ., data = train) |>
  recipes::step_center(c(2, 6, 7, 15)) |> 
  recipes::step_scale(c(2, 6, 7, 15)) 
```

### Define model control

Next step we need to do is define the model control.
At this step we can define for example what kind of summary function should be used (what kind of metrics will be computed), define the resampling method and its parameters.

```{r control}
ctrl <- caret::trainControl(method = "cv",
                     number = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary
                     )
```

### Define custom parameter tuning

Next we may want to define tuning parameters.
For this `expand.grid()` function is the best, since it will create grid of all the combination of individual values of parameters.
In case of large number of parameters, or long computational time, it is good to use some space filling algorithm from the field of design of experiments.
Many packages concerned with this can be found in CRAN [task view](https://cran.r-project.org/web/views/ExperimentalDesign.html)

```{r tuning}
tune_params <- expand.grid(colsample_bytree = 0.7,
                          eta = 0.3,
                          subsample = 0.5,
                          nrounds = 100,
                          gamma = c(0.5, 0.75),
                          min_child_weight = c(1, 2),
                          max_depth = 6)
```
### Train the model

Finally, we will put all the things together and train the model. 
Depending on the option we choose, several models will be trained and the best one will be chosen for usage in prediction.
In train function we choose the model we want to use and metric which should be used for model evaluation.

```{r training}
set.seed(5627)
xgb_model_clean <- train(rec_obj,
                         data = train,
                         method = "xgbTree",
                         metric = "ROC",
                         trControl = ctrl,
                         verbosity = 0,
                         tuneGrid = tune_params
)
```
## Handling class imbalance

The Key Indicators of Heart Disease dataset classes are extremely unbalanced, due to this the learning and evaluated metrics are not performing best.
There are several ways how to handle unbalanced classes, we could use custom function to compute metric, or we could try to upsample the observation a bit by creating artificial observation for the smaller class.

Now we will run

Rose and SMOTE drop observations with NAs
```{r rose}
ctrl$sampling <- "rose"

set.seed(5627)
xgb_model_rose <- train(rec_obj,
                   data = train,
                   method = "xgbTree",
                   metric = "ROC",
                   trControl = ctrl,
                   verbosity = 0,
                   tuneGrid = tune_params
                   ) 
```

```{r smote}
ctrl$sampling <- "smote"

set.seed(5627)
xgb_model_smote <- train(rec_obj,
                   data = train,
                   method = "xgbTree",
                   metric = "ROC",
                   trControl = ctrl,
                   verbosity = 0,
                   tuneGrid = tune_params
                   ) 
```
## Prediction and comparison

```{r prediction, exercise = TRUE}
final_models <- list(original = xgb_model_clean,
                     SMOTE = xgb_model_smote,
                     ROSE = xgb_model_rose)

resampling <- caret::resamples(final_models)

prediction <- final_models |>
  lapply(courses.it4i::test_roc,
         data = test[, -1],
         y = as.numeric(test[[1]]))

prediction <- lapply(prediction, as.vector)
prediction <- do.call("rbind", prediction)
colnames(prediction) <- c("lower", "ROC", "upper")
prediction <- as.data.frame(prediction)

summary(resampling, metric = "ROC")
prediction
```
## Cheatsheet

- `caret::createDataPartition()` - get indices for the training set
- `recipe()` - start a recipe
- `step_center()` - add centering step
- `step_scale()` - add scaling step
- `caret::trainControl()` - setup train control

## Exercises

Now You can try to build a perfect model, for the credit prediction `credit_data` from package [modeldata](https://cran.r-project.org/package=modeldata).
You need to first transform factors into numeric, then create training and testing split.
Define the recipe for the data transformation and train the model.
Check the model predictive capabilities. 
You can use `caret::confusionMatrix()`, or the way similar to the one used before, when multiple models are trained.
The target variable is Status, which can be either "bad", or "good". This shall remain as a factor, all the other variables should be numeric.
Remember the tuning variables for "xgbtree" are colsample_bytree, eta, subsample, nrounds, gamma, min_child_weight, max_depth.

```{r credit_data, exercise = TRUE, exercise.lines = 30}
credit_data
```

## Useful resources

I found inspiration, or plainly used the same examples as are on other sites and would like to use this place to link them here. (I deeply apologize to all the ones I might have forgotten about.)
Most of them contain much more details about individual parts that were covered by this short tutorial.

- caret github - [https://github.com/topepo/caret](https://github.com/topepo/caret)
- recipes page - [https://recipes.tidymodels.org](https://recipes.tidymodels.org)
- Kaggle data - [https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)
- xgboost page - [https://xgboost.readthedocs.io/en/stable/R-package/index.html](https://xgboost.readthedocs.io/en/stable/R-package/index.html)
- pROC github - [https://github.com/xrobin/pROC](https://github.com/xrobin/pROC)
- roc wikipedia page - [https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
- xgboost model explanation - [https://xgboost.readthedocs.io/en/stable/tutorials/model.html](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)
- SMOTE explanation - [https://towardsdatascience.com/smote-fdce2f605729](https://towardsdatascience.com/smote-fdce2f605729)
